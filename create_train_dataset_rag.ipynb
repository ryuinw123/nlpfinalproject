{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "facd189c-32e5-406a-9f84-5ab8f04347a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 13:03:09.470424: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744808589.481717   54505 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744808589.485161   54505 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-16 13:03:09.498020: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from qdrant_client import QdrantClient, models\n",
    "from langchain_qdrant import FastEmbedSparse, QdrantVectorStore, RetrievalMode\n",
    "from qdrant_client.http.models import Distance, SparseVectorParams, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from uuid import uuid4\n",
    "import json\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bc9e7ed-4fef-4347-a6fd-004c552dec11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36acbb7b1bd443a85cf1bdababaf3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af01c37d0b18481d818487cc94de80e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98a8a657b9f415dadf4ee002ef4f2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqa_dataset = load_dataset(\"mandarjoshi/trivia_qa\" , \"rc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd918620-ef2e-4119-b854-23b0d2142ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqa_dataset_train = tqa_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f3bf785-1137-4423-8773-a4fa694df4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"wikipedia-dot\"\n",
    "client = QdrantClient(url=\"http://192.168.2.3:6333\" , timeout=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2df787-f6e4-4738-ac2a-aa58be5cbf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('nthakur/contriever-base-msmarco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac1cf97-2342-407d-a8d2-bc52253dd955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def batched(iterable, n):\n",
    "    iterator = iter(iterable)\n",
    "    while batch := list(islice(iterator, n)):\n",
    "        yield batch\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e024056-c478-41f9-b22c-70af74e6533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def get_overlap(s1, s2):\n",
    "    s = difflib.SequenceMatcher(None, s1, s2)\n",
    "    pos_a, pos_b, size = s.find_longest_match(0, len(s1), 0, len(s2)) \n",
    "    return s1[pos_a:pos_a+size]\n",
    "\n",
    "def is_gold_compression(text, supporting_map, threshold=0.5):\n",
    "    text = text.lower().strip()\n",
    "    for supporting_text in supporting_map:\n",
    "        supporting_text = supporting_text.lower().strip()\n",
    "        if not supporting_text or not text:\n",
    "            continue\n",
    "        overlap = get_overlap(supporting_text, text)\n",
    "        overlap_len = len(overlap)\n",
    "        if (\n",
    "            (overlap_len / len(supporting_text) > threshold)\n",
    "            or\n",
    "            (overlap_len / len(text) > threshold)\n",
    "        ):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "893ccf21-5302-4429-9ced-bb3f37ea787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# total_batches = math.ceil(len(tqa_dataset_train) / batch_size)\n",
    "\n",
    "# document_tqa_train = []\n",
    "\n",
    "# for batch in tqdm(batched(tqa_dataset_train, batch_size), total=total_batches, desc=\"dataset batches\"):\n",
    "#     questions = [data[\"question\"] for data in batch]\n",
    "#     answers = [data[\"answer\"] for data in batch]\n",
    "\n",
    "#     # Lowercased answer sets\n",
    "#     answers_grouped = [\n",
    "#         set(s.lower() for s in ans[\"aliases\"] + ans[\"normalized_aliases\"] + [ans[\"normalized_trainue\"]])\n",
    "#         for ans in answers\n",
    "#     ]\n",
    "#     for group in answers_grouped:\n",
    "#         group_copy = {s for s in group if len(s.strip()) >= 2}\n",
    "#         group.clear()\n",
    "#         group.update(group_copy)\n",
    "\n",
    "\n",
    "#     # Encode questions\n",
    "#     queries_encode = model.encode(questions)\n",
    "\n",
    "#     # Prepare batch search\n",
    "#     search_queries = [\n",
    "#         models.QueryRequest(query=query, with_payload=True, limit=100)\n",
    "#         for query in queries_encode\n",
    "#     ]\n",
    "\n",
    "#     # Run batch query\n",
    "#     batch_point = client.query_batch_points(collection_name=collection_name, requests=search_queries)\n",
    "\n",
    "#     # Process results\n",
    "#     for i, query in enumerate(batch_point):\n",
    "#         ctxs = []\n",
    "#         for point in query.points:\n",
    "#             doc_id = point.payload.get(\"docid\")\n",
    "#             title = point.payload.get(\"title\")\n",
    "#             text = point.payload.get(\"text\")\n",
    "#             score = point.score\n",
    "\n",
    "#             # Match answer aliases in text\n",
    "#             matches = {\n",
    "#                 (match.start(), len(match.group()))\n",
    "#                 for alias in answers_grouped[i]\n",
    "#                 for match in re.finditer(re.escape(alias), text.lower())\n",
    "#             }\n",
    "\n",
    "#             has_answer = len(matches) > 0\n",
    "\n",
    "#             ctxs.append({\n",
    "#                 \"id\": doc_id,\n",
    "#                 \"title\": title,\n",
    "#                 \"text\": text,\n",
    "#                 \"score\": score,\n",
    "#                 \"has_answer\": has_answer,\n",
    "#                 \"answer_occurrences\": [\n",
    "#                     {\"start\": start, \"length\": length} for (start, length) in matches\n",
    "#                 ]\n",
    "#             })\n",
    "\n",
    "#         document_tqa_train.append({\n",
    "#             \"question\": questions[i],\n",
    "#             \"answers\": list(answers_grouped[i]),\n",
    "#             \"ctxs\": ctxs\n",
    "#         })\n",
    "\n",
    "# with open(\"dataset/document_tqa_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(document_tqa_train, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05015a43-d819-4bd8-8db0-e1e975a9cebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ataset batches: 100%|████████████████████████████████████████████████████████████| 2617/2617 [1:21:03<00:00,  1.86s/it]"
     ]
    }
   ],
   "source": [
    "two_wiki_qa = load_dataset(\"kamelliao/2wikimultihopqa\")\n",
    "two_wiki_qa_dataset_train = two_wiki_qa[\"train\"]\n",
    "two_wiki_qa_dataset_train = two_wiki_qa_dataset_train.filter(\n",
    "    lambda example: all(title in example['context']['title'] for title in set(example['supporting_facts']['title']))\n",
    ")\n",
    "total_batches = math.ceil(len(two_wiki_qa_dataset_train) / batch_size)\n",
    "\n",
    "document_two_wiki_qa_train = []\n",
    "\n",
    "for batch in tqdm(batched(two_wiki_qa_dataset_train, batch_size), total=total_batches, desc=\"dataset batches\"):\n",
    "    questions = [data[\"question\"] for data in batch]\n",
    "    answers = [data[\"answer\"] for data in batch]\n",
    "    \n",
    "    # Lowercased answer sets\n",
    "    answers_grouped = [\n",
    "        set(ans.lower())\n",
    "        for ans in answers\n",
    "    ]\n",
    "\n",
    "    for group in answers_grouped:\n",
    "        group_copy = {s for s in group if len(s.strip()) >= 2}\n",
    "        group.clear()\n",
    "        group.update(group_copy)\n",
    "\n",
    "    # Encode questions\n",
    "    queries_encode = model.encode(questions)\n",
    "\n",
    "    # Prepare batch search\n",
    "    search_queries = [\n",
    "        models.QueryRequest(query=query, with_payload=True, limit=100)\n",
    "        for query in queries_encode\n",
    "    ]\n",
    "\n",
    "    # Run batch query\n",
    "    batch_point = client.query_batch_points(collection_name=collection_name, requests=search_queries)\n",
    "\n",
    "    # Process results\n",
    "    for i, query in enumerate(batch_point):\n",
    "        support_titles = batch[i][\"supporting_facts\"][\"title\"]\n",
    "        support_sent_ids = batch[i][\"supporting_facts\"][\"sent_id\"]\n",
    "        context_titles = batch[i][\"context\"][\"title\"]\n",
    "        context_sentences = batch[i][\"context\"][\"sentences\"]\n",
    "        \n",
    "        # Build supporting sentence map per title\n",
    "        supporting_map = []\n",
    "        for title, sent_id in zip(support_titles, support_sent_ids):\n",
    "            try:\n",
    "                title_idx = context_titles.index(title)\n",
    "                sentence = context_sentences[title_idx][sent_id]\n",
    "                supporting_map.append(sentence.strip())\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "        ctxs = []\n",
    "        for point in query.points:\n",
    "            doc_id = point.payload.get(\"docid\")\n",
    "            title = point.payload.get(\"title\")\n",
    "            text = point.payload.get(\"text\")\n",
    "            score = point.score\n",
    "\n",
    "            # Match answer aliases in text\n",
    "            matches = {\n",
    "                (match.start(), len(match.group()))\n",
    "                for alias in answers_grouped[i]\n",
    "                for match in re.finditer(re.escape(alias), text.lower())\n",
    "            }\n",
    "\n",
    "            gold_document = is_gold_compression(text, supporting_map)\n",
    "\n",
    "            has_answer = len(matches) > 0\n",
    "\n",
    "            ctxs.append({\n",
    "                \"id\": doc_id,\n",
    "                \"title\": title,\n",
    "                \"text\": text,\n",
    "                \"score\": score,\n",
    "                \"has_answer\": has_answer,\n",
    "                \"answer_occurrences\": [\n",
    "                    {\"start\": start, \"length\": length} for (start, length) in matches\n",
    "                ],\n",
    "                \"gold_document\" : gold_document\n",
    "            })\n",
    "\n",
    "        document_two_wiki_qa_train.append({\n",
    "            \"question\": questions[i],\n",
    "            \"answers\": list(answers_grouped[i]),\n",
    "            \"ctxs\": ctxs\n",
    "        })\n",
    "\n",
    "with open(\"dataset/document_two_wiki_qa_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(document_two_wiki_qa_train, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
