{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ff1561-c3fa-4b19-b324-b32463c556fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facd189c-32e5-406a-9f84-5ab8f04347a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 22:38:04.107392: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745966284.118599    3847 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745966284.122362    3847 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-29 22:38:04.134849: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from qdrant_client import QdrantClient, models\n",
    "from langchain_qdrant import FastEmbedSparse, QdrantVectorStore, RetrievalMode\n",
    "from qdrant_client.http.models import Distance, SparseVectorParams, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from uuid import uuid4\n",
    "import json\n",
    "\n",
    "import math\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f3bf785-1137-4423-8773-a4fa694df4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"wikipedia-dot\"\n",
    "client = QdrantClient(url=\"http://192.168.2.3:6333\" , timeout=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2df787-f6e4-4738-ac2a-aa58be5cbf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('nthakur/contriever-base-msmarco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ac1cf97-2342-407d-a8d2-bc52253dd955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def batched(iterable, n):\n",
    "    iterator = iter(iterable)\n",
    "    while batch := list(islice(iterator, n)):\n",
    "        yield batch\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6b55e8-1069-45b4-b100-0fa640606211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def get_overlap(s1, s2):\n",
    "    s = difflib.SequenceMatcher(None, s1, s2)\n",
    "    pos_a, pos_b, size = s.find_longest_match(0, len(s1), 0, len(s2)) \n",
    "    return s1[pos_a:pos_a+size]\n",
    "\n",
    "def is_gold_compression(text, supporting_map, threshold=0.5):\n",
    "    text = text.lower().strip()\n",
    "    for supporting_text in supporting_map:\n",
    "        supporting_text = supporting_text.lower().strip()\n",
    "        if not supporting_text or not text:\n",
    "            continue\n",
    "        overlap = get_overlap(supporting_text, text)\n",
    "        overlap_len = len(overlap)\n",
    "        if (\n",
    "            (overlap_len / len(supporting_text) > threshold)\n",
    "            or\n",
    "            (overlap_len / len(text) > threshold)\n",
    "        ):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79c3bc28-32a2-4b19-a713-8dcceed82878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqa_dataset = load_dataset(\"mandarjoshi/trivia_qa\" , \"rc\")\n",
    "# tqa_dataset_val = tqa_dataset[\"validation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "893ccf21-5302-4429-9ced-bb3f37ea787d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b67c327c204b8cad4d0e80c2a6f013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e92965c41ed446a8383ee2742c79057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862c35c229bd4e8a931c31bc88b4e856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/26 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84456625953143279991219c4982b05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/138384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a447b863ed44ee5b89ba5b15754b38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/17944 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e02468814f64d96ae8d2dc74745713d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/17210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1535388c37e1456ca76dbe4004f2a4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset batches: 100%|████████████████████████████████████████████████████████████████| 281/281 [06:07<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "tqa_dataset = load_dataset(\"mandarjoshi/trivia_qa\" , \"rc\")\n",
    "tqa_dataset_val = tqa_dataset[\"validation\"]\n",
    "total_batches = math.ceil(len(tqa_dataset_val) / batch_size)\n",
    "\n",
    "document_tqa_test = []\n",
    "\n",
    "for batch in tqdm(batched(tqa_dataset_val, batch_size), total=total_batches, desc=\"dataset batches\"):\n",
    "    questions = [data[\"question\"] for data in batch]\n",
    "    answers = [data[\"answer\"] for data in batch]\n",
    "\n",
    "    # Lowercased answer sets\n",
    "    answers_grouped = [\n",
    "        set(s.lower() for s in ans[\"aliases\"] + ans[\"normalized_aliases\"] + [ans[\"normalized_value\"]])\n",
    "        for ans in answers\n",
    "    ]\n",
    "\n",
    "    # Encode questions\n",
    "    queries_encode = model.encode(questions)\n",
    "\n",
    "    # Prepare batch search\n",
    "    search_queries = [\n",
    "        models.QueryRequest(query=query, with_payload=True, limit=100)\n",
    "        for query in queries_encode\n",
    "    ]\n",
    "\n",
    "    # Run batch query\n",
    "    batch_point = client.query_batch_points(collection_name=collection_name, requests=search_queries)\n",
    "\n",
    "    # Process results\n",
    "    for i, query in enumerate(batch_point):\n",
    "        ctxs = []\n",
    "        for point in query.points:\n",
    "            doc_id = point.payload.get(\"docid\")\n",
    "            title = point.payload.get(\"title\")\n",
    "            text = point.payload.get(\"text\")\n",
    "            score = point.score\n",
    "\n",
    "            # Match answer aliases in text\n",
    "            matches = {\n",
    "                (match.start(), len(match.group()))\n",
    "                for alias in answers_grouped[i]\n",
    "                for match in re.finditer(re.escape(alias), text.lower())\n",
    "            }\n",
    "\n",
    "            has_answer = len(matches) > 0\n",
    "\n",
    "            ctxs.append({\n",
    "                \"id\": doc_id,\n",
    "                \"title\": title,\n",
    "                \"text\": text,\n",
    "                \"score\": score,\n",
    "                \"has_answer\": has_answer,\n",
    "                \"answer_occurrences\": [\n",
    "                    {\"start\": start, \"length\": length} for (start, length) in matches\n",
    "                ]\n",
    "            })\n",
    "\n",
    "        document_tqa_test.append({\n",
    "            \"question\": questions[i],\n",
    "            \"answers\": list(answers_grouped[i]),\n",
    "            \"ctxs\": ctxs\n",
    "        })\n",
    "\n",
    "with open(\"dataset/test_tqa.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(document_tqa_test, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17ff9078-d529-40ba-94ce-e1006e143680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hotpot_qa = load_dataset(\"hotpotqa/hotpot_qa\" , \"fullwiki\")\n",
    "# hotpot_qa_dataset_val = hotpot_qa[\"validation\"]\n",
    "# hotpot_qa_dataset_val = hotpot_qa_dataset_val.filter(\n",
    "#     lambda example: all(title in example['context']['title'] for title in set(example['supporting_facts']['title']))\n",
    "# )\n",
    "# total_batches = math.ceil(len(hotpot_qa_dataset_val) / batch_size)\n",
    "\n",
    "# document_hotpotqa_test = []\n",
    "\n",
    "# for batch in tqdm(batched(hotpot_qa_dataset_val, batch_size), total=total_batches, desc=\"dataset batches\"):\n",
    "#     questions = [data[\"question\"] for data in batch]\n",
    "#     answers = [data[\"answer\"] for data in batch]\n",
    "\n",
    "#     # Lowercased answer sets\n",
    "#     answers_grouped = [\n",
    "#         set([ans.lower()])\n",
    "#         for ans in answers\n",
    "#     ]\n",
    "\n",
    "#     # Encode questions\n",
    "#     queries_encode = model.encode(questions)\n",
    "\n",
    "#     # Prepare batch search\n",
    "#     search_queries = [\n",
    "#         models.QueryRequest(query=query, with_payload=True, limit=100)\n",
    "#         for query in queries_encode\n",
    "#     ]\n",
    "\n",
    "#     # Run batch query\n",
    "#     batch_point = client.query_batch_points(collection_name=collection_name, requests=search_queries)\n",
    "\n",
    "#     # Process results\n",
    "#     for i, query in enumerate(batch_point):\n",
    "\n",
    "        \n",
    "#         support_titles = batch[i][\"supporting_facts\"][\"title\"]\n",
    "#         support_sent_ids = batch[i][\"supporting_facts\"][\"sent_id\"]\n",
    "#         context_titles = batch[i][\"context\"][\"title\"]\n",
    "#         context_sentences = batch[i][\"context\"][\"sentences\"]\n",
    "        \n",
    "#         # Build supporting sentence map per title\n",
    "#         supporting_map = []\n",
    "#         for title, sent_id in zip(support_titles, support_sent_ids):\n",
    "#             try:\n",
    "#                 title_idx = context_titles.index(title)\n",
    "#                 sentence = context_sentences[title_idx][sent_id]\n",
    "#                 supporting_map.append(sentence.strip())\n",
    "#             except (ValueError, IndexError):\n",
    "#                 continue\n",
    "\n",
    "#         ctxs = []\n",
    "#         for point in query.points:\n",
    "#             doc_id = point.payload.get(\"docid\")\n",
    "#             title = point.payload.get(\"title\")\n",
    "#             text = point.payload.get(\"text\")\n",
    "#             score = point.score\n",
    "\n",
    "#             # Match answer aliases in text\n",
    "#             matches = {\n",
    "#                 (match.start(), len(match.group()))\n",
    "#                 for alias in answers_grouped[i]\n",
    "#                 for match in re.finditer(re.escape(alias), text.lower())\n",
    "#             }\n",
    "\n",
    "#             has_answer = len(matches) > 0\n",
    "\n",
    "#             gold_document = is_gold_compression(text, supporting_map)\n",
    "\n",
    "#             ctxs.append({\n",
    "#                 \"id\": doc_id,\n",
    "#                 \"title\": title,\n",
    "#                 \"text\": text,\n",
    "#                 \"score\": score,\n",
    "#                 \"has_answer\": has_answer,\n",
    "#                 \"answer_occurrences\": [\n",
    "#                     {\"start\": start, \"length\": length} for (start, length) in matches\n",
    "#                 ],\n",
    "#                 \"gold_document\" : gold_document\n",
    "#             })\n",
    "\n",
    "#         document_hotpotqa_test.append({\n",
    "#             \"question\": questions[i],\n",
    "#             \"answers\": list(answers_grouped[i]),\n",
    "#             \"ctxs\": ctxs\n",
    "#         })\n",
    "\n",
    "# with open(\"dataset/test_hotpotqa.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(document_hotpotqa_test, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a7beb2c-6306-4d43-b125-02f12d5ef4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_dataset(sample):\n",
    "#     question = sample['question']['text']\n",
    "#     context = sample['document']['tokens']['token']\n",
    "#     is_html = sample['document']['tokens']['is_html']\n",
    "#     long_answers = sample['annotations']['long_answer']\n",
    "#     short_answers = sample['annotations']['short_answers']\n",
    "    \n",
    "#     context_string =  \" \".join([context[i] for i in range(len(context)) if not is_html[i]])\n",
    "    \n",
    "#     # 0 - No ; 1 - Yes\n",
    "#     for answer in sample['annotations']['yes_no_answer']:\n",
    "#         if answer == 0 or answer == 1:\n",
    "#           return {\"question\": question, \"context\": context_string, \"short\": [], \"long\": [], \"category\": \"no\" if answer == 0 else \"yes\" , \"answer\" : [\"no\"] if answer == 0 else [\"yes\"]}\n",
    "    \n",
    "#     short_targets = []\n",
    "#     for s in short_answers:\n",
    "#         short_targets.extend(s['text'])\n",
    "#     short_targets = list(set(short_targets))\n",
    "    \n",
    "#     long_targets = []\n",
    "#     for s in long_answers:\n",
    "#         if s['start_token'] == -1:\n",
    "#             continue\n",
    "#         answer = context[s['start_token']: s['end_token']]\n",
    "#         html = is_html[s['start_token']: s['end_token']]\n",
    "#         new_answer = \" \".join([answer[i] for i in range(len(answer)) if not html[i]])\n",
    "#         if new_answer not in long_targets:\n",
    "#             long_targets.append(new_answer)\n",
    "    \n",
    "#     category = \"long_short\" if len(short_targets + long_targets) > 0 else \"null\"\n",
    "    \n",
    "#     return {\"question\": question, \"context\": context_string, \"short\": short_targets, \"long\": long_targets, \"category\": category , \"answer\" : short_targets + long_targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a77c4ada-fb0d-47b3-9796-c6e34a9919f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural_qa = load_dataset(\"google-research-datasets/natural_questions\")\n",
    "# natural_qa_dataset_val = natural_qa[\"validation\"]\n",
    "# natural_qa_dataset_val = natural_qa_dataset_val.map(format_dataset).remove_columns([\"annotations\", \"document\", \"id\"])\n",
    "# natural_qa_dataset_val = natural_qa_dataset_val.filter(lambda x: x[\"category\"] != \"null\")\n",
    "# total_batches = math.ceil(len(natural_qa_dataset_val) / batch_size)\n",
    "\n",
    "# document_natural_qa_test = []\n",
    "\n",
    "# for batch in tqdm(batched(natural_qa_dataset_val, batch_size), total=total_batches, desc=\"dataset batches\"):\n",
    "#     questions = [data[\"question\"] for data in batch]\n",
    "#     answers = [data[\"answer\"] for data in batch]\n",
    "\n",
    "#     # Lowercased answer sets\n",
    "#     answers_grouped = [\n",
    "#         set(s.lower() for s in ans)\n",
    "#         for ans in answers\n",
    "#     ]\n",
    "\n",
    "#     # Encode questions\n",
    "#     queries_encode = model.encode(questions)\n",
    "\n",
    "#     # Prepare batch search\n",
    "#     search_queries = [\n",
    "#         models.QueryRequest(query=query, with_payload=True, limit=100)\n",
    "#         for query in queries_encode\n",
    "#     ]\n",
    "\n",
    "#     # Run batch query\n",
    "#     batch_point = client.query_batch_points(collection_name=collection_name, requests=search_queries)\n",
    "\n",
    "#     # Process results\n",
    "#     for i, query in enumerate(batch_point):\n",
    "#         ctxs = []\n",
    "#         for point in query.points:\n",
    "#             doc_id = point.payload.get(\"docid\")\n",
    "#             title = point.payload.get(\"title\")\n",
    "#             text = point.payload.get(\"text\")\n",
    "#             score = point.score\n",
    "\n",
    "#             # Match answer aliases in text\n",
    "#             matches = {\n",
    "#                 (match.start(), len(match.group()))\n",
    "#                 for alias in answers_grouped[i]\n",
    "#                 for match in re.finditer(re.escape(alias), text.lower())\n",
    "#             }\n",
    "\n",
    "#             has_answer = len(matches) > 0\n",
    "\n",
    "#             ctxs.append({\n",
    "#                 \"id\": doc_id,\n",
    "#                 \"title\": title,\n",
    "#                 \"text\": text,\n",
    "#                 \"score\": score,\n",
    "#                 \"has_answer\": has_answer,\n",
    "#                 \"answer_occurrences\": [\n",
    "#                     {\"start\": start, \"length\": length} for (start, length) in matches\n",
    "#                 ]\n",
    "#             })\n",
    "\n",
    "#         document_natural_qa_test.append({\n",
    "#             \"question\": questions[i],\n",
    "#             \"answers\": list(answers_grouped[i]),\n",
    "#             \"ctxs\": ctxs\n",
    "#         })\n",
    "\n",
    "# with open(\"dataset/test_nqa.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(document_natural_qa_test, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96811c75-d9e2-4e71-abed-fe47e046c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# musique_qa = load_dataset(\"dgslibisey/MuSiQue\")\n",
    "# musique_qa_dataset_val = musique_qa[\"validation\"]\n",
    "# musique_qa_dataset_val = musique_qa_dataset_val.filter(lambda x : x[\"answerable\"] == True)\n",
    "# total_batches = math.ceil(len(musique_qa_dataset_val) / batch_size)\n",
    "\n",
    "# document_musique_qa_test = []\n",
    "\n",
    "# for batch in tqdm(batched(musique_qa_dataset_val, batch_size), total=total_batches, desc=\"dataset batches\"):\n",
    "#     questions = [data[\"question\"] for data in batch]\n",
    "#     answers = [[data[\"answer\"]] + data[\"answer_aliases\"] for data in batch]\n",
    "\n",
    "\n",
    "#     # Lowercased answer sets\n",
    "#     answers_grouped = [\n",
    "#         set(s.lower() for s in ans)\n",
    "#         for ans in answers\n",
    "#     ]\n",
    "\n",
    "#     # Encode questions\n",
    "#     queries_encode = model.encode(questions)\n",
    "\n",
    "#     # Prepare batch search\n",
    "#     search_queries = [\n",
    "#         models.QueryRequest(query=query, with_payload=True, limit=100)\n",
    "#         for query in queries_encode\n",
    "#     ]\n",
    "\n",
    "#     # Run batch query\n",
    "#     batch_point = client.query_batch_points(collection_name=collection_name, requests=search_queries)\n",
    "\n",
    "#     # Process results\n",
    "#     for i, query in enumerate(batch_point):\n",
    "\n",
    "#         supporting_map = [doc[\"paragraph_text\"] for doc in batch[i][\"paragraphs\"] if doc[\"is_supporting\"] == True]\n",
    "        \n",
    "#         ctxs = []\n",
    "#         for point in query.points:\n",
    "#             doc_id = point.payload.get(\"docid\")\n",
    "#             title = point.payload.get(\"title\")\n",
    "#             text = point.payload.get(\"text\")\n",
    "#             score = point.score\n",
    "\n",
    "#             # Match answer aliases in text\n",
    "#             matches = {\n",
    "#                 (match.start(), len(match.group()))\n",
    "#                 for alias in answers_grouped[i]\n",
    "#                 for match in re.finditer(re.escape(alias), text.lower())\n",
    "#             }\n",
    "\n",
    "#             has_answer = len(matches) > 0\n",
    "\n",
    "#             gold_document = is_gold_compression(text, supporting_map)\n",
    "#             ctxs.append({\n",
    "#                 \"id\": doc_id,\n",
    "#                 \"title\": title,\n",
    "#                 \"text\": text,\n",
    "#                 \"score\": score,\n",
    "#                 \"has_answer\": has_answer,\n",
    "#                 \"answer_occurrences\": [\n",
    "#                     {\"start\": start, \"length\": length} for (start, length) in matches\n",
    "#                 ],\n",
    "#                 \"gold_document\" : gold_document\n",
    "#             })\n",
    "\n",
    "#         document_musique_qa_test.append({\n",
    "#             \"question\": questions[i],\n",
    "#             \"answers\": list(answers_grouped[i]),\n",
    "#             \"ctxs\": ctxs\n",
    "#         })\n",
    "\n",
    "# with open(\"dataset/test_musique.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(document_musique_qa_test, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a474544-4302-4446-b5ca-a456bb37e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two_wiki_qa = load_dataset(\"kamelliao/2wikimultihopqa\")\n",
    "# two_wiki_qa_dataset_val = two_wiki_qa[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eb654ac-7795-495d-8e6d-a4f12b9e357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two_wiki_qa_dataset_val[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec647c39-14c9-4a55-bb31-07de59f42a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two_wiki_qa = load_dataset(\"kamelliao/2wikimultihopqa\")\n",
    "# two_wiki_qa_dataset_val = two_wiki_qa[\"validation\"]\n",
    "# two_wiki_qa_dataset_val = two_wiki_qa_dataset_val.filter(\n",
    "#     lambda example: all(title in example['context']['title'] for title in set(example['supporting_facts']['title']))\n",
    "# )\n",
    "# total_batches = math.ceil(len(two_wiki_qa_dataset_val) / batch_size)\n",
    "\n",
    "# document_two_wiki_qa_test = []\n",
    "\n",
    "# for batch in tqdm(batched(two_wiki_qa_dataset_val, batch_size), total=total_batches, desc=\"dataset batches\"):\n",
    "#     questions = [data[\"question\"] for data in batch]\n",
    "#     answers = [data[\"answer\"] for data in batch]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#     # Lowercased answer sets\n",
    "#     answers_grouped = [\n",
    "#         set([ans.lower()])\n",
    "#         for ans in answers\n",
    "#     ]\n",
    "\n",
    "\n",
    "#     # Encode questions\n",
    "#     queries_encode = model.encode(questions)\n",
    "\n",
    "#     # Prepare batch search\n",
    "#     search_queries = [\n",
    "#         models.QueryRequest(query=query, with_payload=True, limit=100)\n",
    "#         for query in queries_encode\n",
    "#     ]\n",
    "\n",
    "#     # Run batch query\n",
    "#     batch_point = client.query_batch_points(collection_name=collection_name, requests=search_queries)\n",
    "\n",
    "#     # Process results\n",
    "#     for i, query in enumerate(batch_point):\n",
    "#         support_titles = batch[i][\"supporting_facts\"][\"title\"]\n",
    "#         support_sent_ids = batch[i][\"supporting_facts\"][\"sent_id\"]\n",
    "#         context_titles = batch[i][\"context\"][\"title\"]\n",
    "#         context_sentences = batch[i][\"context\"][\"sentences\"]\n",
    "        \n",
    "#         # Build supporting sentence map per title\n",
    "#         supporting_map = []\n",
    "#         for title, sent_id in zip(support_titles, support_sent_ids):\n",
    "#             try:\n",
    "#                 title_idx = context_titles.index(title)\n",
    "#                 sentence = context_sentences[title_idx][sent_id]\n",
    "#                 supporting_map.append(sentence.strip())\n",
    "#             except (ValueError, IndexError):\n",
    "#                 continue\n",
    "#         ctxs = []\n",
    "#         for point in query.points:\n",
    "#             doc_id = point.payload.get(\"docid\")\n",
    "#             title = point.payload.get(\"title\")\n",
    "#             text = point.payload.get(\"text\")\n",
    "#             score = point.score\n",
    "\n",
    "#             # Match answer aliases in text\n",
    "#             matches = {\n",
    "#                 (match.start(), len(match.group()))\n",
    "#                 for alias in answers_grouped[i]\n",
    "#                 for match in re.finditer(re.escape(alias), text.lower())\n",
    "#             }\n",
    "\n",
    "#             gold_document = is_gold_compression(text, supporting_map)\n",
    "\n",
    "#             has_answer = len(matches) > 0\n",
    "\n",
    "#             ctxs.append({\n",
    "#                 \"id\": doc_id,\n",
    "#                 \"title\": title,\n",
    "#                 \"text\": text,\n",
    "#                 \"score\": score,\n",
    "#                 \"has_answer\": has_answer,\n",
    "#                 \"answer_occurrences\": [\n",
    "#                     {\"start\": start, \"length\": length} for (start, length) in matches\n",
    "#                 ],\n",
    "#                 \"gold_document\" : gold_document\n",
    "#             })\n",
    "\n",
    "#         document_two_wiki_qa_test.append({\n",
    "#             \"question\": questions[i],\n",
    "#             \"answers\": list(answers_grouped[i]),\n",
    "#             \"ctxs\": ctxs\n",
    "#         })\n",
    "\n",
    "# with open(\"dataset/test_2wiki.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(document_two_wiki_qa_test, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04dfecef-2a3b-436b-8aef-9131fb8ea609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_two_wiki_qa_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f5bcdd-9641-4d89-855b-88c3a154d3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
